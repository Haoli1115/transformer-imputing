{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.models import Model\n",
    "import statsmodels.api as sm\n",
    "from math import sqrt as math_sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Import custom modules\n",
    "sys.path.append('../')\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.4.1\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data, testing_index = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries, no_features, feature_names, years, months, weekdays, hours = load_data_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts features, year, etc. from whole data\n",
    "def change_format(input_data):\n",
    "    #Extract year from data matrix\n",
    "    year_w = input_data[:,:,0:1]\n",
    "    #Extract weekday from data matrix\n",
    "    weekday_w = input_data[:,:,1:2]\n",
    "    #Extract hour from data matrix\n",
    "    hour_w = input_data[:,:,2:3]\n",
    "    #Extract country from data matrix\n",
    "    country_w = input_data[:,0:1,3]   \n",
    "    #Extract month from data matrix\n",
    "    month_w = input_data[:,:,4:5]\n",
    "    #Extract features from matrix\n",
    "    features_w = input_data[:,:,5:5+no_features]\n",
    "    #Extract matrix of missing values from data matrix\n",
    "    miss_vals_w = input_data[:,:,-no_features-6:-6]\n",
    "    #Extract pos enc from data matrix\n",
    "    pos_enc_w = input_data[:,:,-6:]\n",
    "\n",
    "\n",
    "    #Prepare format for features\n",
    "    features_tf = np.reshape(features_w, [features_w.shape[0], -1, 1])\n",
    "    miss_vals_tf = np.reshape(miss_vals_w, [features_w.shape[0], -1, 1])\n",
    "    pos_enc_tf = np.reshape(tf.transpose(np.repeat(np.reshape(pos_enc_w, [pos_enc_w.shape[0], pos_enc_w.shape[1], pos_enc_w.shape[2], 1]), no_features, axis = 3), perm=[0,1,3,2]),[pos_enc_w.shape[0],-1,pos_enc_w.shape[2]])\n",
    "    feature_nr_tf = np.repeat(np.reshape(np.repeat(np.reshape(np.array(range(no_features)),[1,-1]), input_data.shape[1], axis = 0),[1,-1]), input_data.shape[0], axis = 0)\n",
    "    \n",
    "    #Reshape other features\n",
    "    hour_tf = np.reshape(np.repeat(hour_w, no_features,axis=2),[input_data.shape[0],-1])\n",
    "    year_tf = np.reshape(np.repeat(year_w, no_features,axis=2),[input_data.shape[0],-1])\n",
    "    weekday_tf = np.reshape(np.repeat(weekday_w, no_features,axis=2),[input_data.shape[0],-1])\n",
    "    month_tf = np.reshape(np.repeat(month_w, no_features,axis=2),[input_data.shape[0],-1])\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    return features_tf, miss_vals_tf, pos_enc_tf, country_w, year_tf, weekday_tf, hour_tf, feature_nr_tf\n",
    "#testing\n",
    "features, miss_vals, pos_enc, country, year, weekday, hour, feature_nr = change_format(training_data[18:19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anpassen der Anzahl der missing values und welches feature gemasked werden soll\n",
    "\n",
    "Features: PV-12 ; Wind-Onshore-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_miss_vals = no_features*12#24 # number of missing values per day (maximum is features masked times 24)\n",
    "masked_feature = 12 # country and year do not count\n",
    "all_masked = 1# if all inserted features should be masked randomly (if 1, masked feature is not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion um Maske Ã¼ber Features zu legen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_features(features, miss_vals):    \n",
    "    mask = np.zeros(features.shape)\n",
    "    for sample in range(features.shape[0]):\n",
    "        if all_masked == 1:        \n",
    "            #Draw number of missing values from Binomial distribution\n",
    "            p = np.random.uniform(0.2,0.8)\n",
    "            n_miss_vals = np.random.binomial(n=24*no_features, p = p)\n",
    "            idx_all = list(range(24*no_features)) # all features masked\n",
    "        else:\n",
    "            n_miss_vals = number_miss_vals\n",
    "            idx_all = list(range(masked_feature,24*no_features,no_features)) # just one feature masked\n",
    "        np.random.shuffle(idx_all)\n",
    "        idx = idx_all[:n_miss_vals]\n",
    "        for mv in range(n_miss_vals):\n",
    "            #Insert mask at the middle day of the week\n",
    "            mask[sample, (2*24)*no_features + idx[mv]] = 1\n",
    "\n",
    "    features_masked = np.array(features)\n",
    "    features_masked[mask==1] = 0\n",
    "\n",
    "    miss_vals_masked = np.array(miss_vals)\n",
    "    miss_vals_masked[mask==1] = 1\n",
    "    \n",
    "\n",
    "    return features_masked, miss_vals_masked, mask\n",
    "features_masked, miss_vals_masked, mask = mask_features(features, miss_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create validation split and fetch data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function for validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_split(data, split = 0.2):\n",
    "    size = int(data.shape[0]*split)\n",
    "    index = data.shape[0]\n",
    "    split_index = random.choices(range(index),k=size)\n",
    "    data_val = data[split_index]\n",
    "    data_train = np.delete(data,split_index,axis=0)\n",
    "    \n",
    "    return data_train, data_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define batch size and create preprocessing for testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        mask = tf.cast(mask, dtype = tf.float32)\n",
    "        scaled_attention_logits = scaled_attention_logits + (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, q, k, v, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights= scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, q, kv, training, mask):\n",
    "\n",
    "        attn_output, attention_weights = self.mha(q, kv, kv, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(attn_output+q)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        \n",
    "    def call(self, q, kv, training, mask):\n",
    "        for i in range(self.num_layers):\n",
    "            q, attention_weights = self.enc_layers[i](q, kv, training, mask)\n",
    "\n",
    "        return q, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristik fÃ¼r Anzahl Embedding AusgÃ¤nge\n",
    "\n",
    "min(600, round(1.6 * (config[\"vocab_size\"] + 1) ** .56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, no_features, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        #Encoding Layers\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, rate)\n",
    "        \n",
    "        #Embedding layers\n",
    "        self.embedding_country = tf.keras.layers.Embedding(countries.shape[0], 9)\n",
    "        self.embedding_year = tf.keras.layers.Embedding(years.shape[0], 4) \n",
    "        #self.embedding_weekday = tf.keras.layers.Embedding(weekdays.shape[0], 4)\n",
    "        #self.embedding_hour = tf.keras.layers.Embedding(hours.shape[0], 8)\n",
    "        self.embedding_feat = tf.keras.layers.Embedding(no_features, 7)\n",
    "        \n",
    "\n",
    "        #Dense Layers\n",
    "        self.first_layer = tf.keras.layers.Dense(d_model)\n",
    "        self.final_layer = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, country, year, weekday, hour, features_masked, miss_vals_masked, \n",
    "            pos_enc, feature_nr, training, enc_padding_mask = None):\n",
    "          \n",
    "        #Embeddings\n",
    "        country_emb = self.embedding_country(country)\n",
    "        year_emb = self.embedding_year(year)\n",
    "        #weekday_emb = self.embedding_weekday(weekday)\n",
    "        #hour_emb = self.embedding_hour(hour)\n",
    "        feat_emb = self.embedding_feat(feature_nr)\n",
    "        \n",
    "        \n",
    "        # concatenation (embeddings plus features)\n",
    "        kv = tf.concat([pos_enc, \n",
    "                        tf.repeat(country_emb, pos_enc.shape[1], axis = 1),\n",
    "                        year_emb, \n",
    "                        #weekday_emb, \n",
    "                        #hour_emb,\n",
    "                        features_masked, miss_vals_masked, feat_emb], axis = 2)\n",
    "        kv = self.first_layer(kv)\n",
    "\n",
    "        q = tf.concat([pos_enc, \n",
    "                       tf.repeat(country_emb, pos_enc.shape[1], axis = 1),\n",
    "                       year_emb, \n",
    "                       #weekday_emb,  \n",
    "                       #hour_emb,\n",
    "                       features_masked, miss_vals_masked, feat_emb], axis = 2)\n",
    "        q = self.first_layer(q)\n",
    "        \n",
    "        enc_output, attention_weights = self.encoder(q, kv, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        final_output = self.final_layer(enc_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss and Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred, mask):\n",
    "    real = real[mask==1]\n",
    "    pred = pred[mask==1]\n",
    "    real = tf.dtypes.cast(real, tf.float32)\n",
    "    error = real-pred\n",
    "    error = tf.square(error)\n",
    "    loss = tf.math.sqrt(tf.math.reduce_mean(error))*1000 # sonst so klein ;-)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 2*d_model\n",
    "num_heads = 1\n",
    "dropout_rate = 0.1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(country, year, weekday, hour, features, features_masked, miss_vals_masked, \n",
    "               pos_enc, feature_nr, mask):\n",
    "\n",
    "    training = True\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred, attention_weights = transformer(country, year, weekday, hour, features_masked, miss_vals_masked, \n",
    "                                              pos_enc, feature_nr,  training)\n",
    "\n",
    "        loss = loss_function(features, pred, mask)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    return loss, pred, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff, no_features, rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing\n",
    "\n",
    "Checkpoint manager kann genutzt werden um trainierte Modelle zu speichern und zu laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/transformer\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler:\n",
    "    def __init__(self, checkpoint, checkpoint_manager, learning_rate, patience = 2, delta = 5, fine_tuning = False, min_epochs = 20):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.delta = delta\n",
    "        self.checkpoint_manager = checkpoint_manager\n",
    "        self.checkpoint = checkpoint\n",
    "        self.learning_rate = learning_rate\n",
    "        self.fine_tuning = fine_tuning\n",
    "        self.min_epochs = min_epochs\n",
    "\n",
    "    def __call__(self, metric, epoch):\n",
    "        #Check if score is not initialized\n",
    "        if self.best_score is None:\n",
    "            self.best_score = metric\n",
    "            return (False,self.learning_rate)\n",
    "        \n",
    "        #Check for minimum amount of epochs\n",
    "        if epoch <= self.min_epochs:\n",
    "            if metric <= self.best_score + self.delta:\n",
    "                self.best_score = metric\n",
    "                ckpt_save_path = self.checkpoint_manager.save()\n",
    "                print(f'Saving checkpoint at {ckpt_save_path}')\n",
    "            return (False, self.learning_rate)\n",
    "        \n",
    "        #If score is worse than before\n",
    "        if metric > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            #If score is still in patience\n",
    "            if self.counter >= self.patience:\n",
    "                #If fine tuning is already running\n",
    "                if self.fine_tuning==True:\n",
    "                    print(\"Training finished\")\n",
    "                    return (True,self.learning_rate)\n",
    "                else:\n",
    "                    self.fine_tuning=True\n",
    "                    self.counter = 0\n",
    "                    self.learning_rate = self.learning_rate/2\n",
    "                    if self.checkpoint_manager.latest_checkpoint:\n",
    "                        self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
    "                        print('Latest checkpoint restored!!')\n",
    "                    print(f'Learning rate adjusted to {np.round(self.learning_rate,6)}')\n",
    "                    self.patience +=2\n",
    "                    return (False,self.learning_rate)\n",
    "            else:                \n",
    "                return (False,self.learning_rate)\n",
    "        #If score is better   \n",
    "        else:\n",
    "            self.best_score = metric\n",
    "            ckpt_save_path = self.checkpoint_manager.save()\n",
    "            print(f'Saving checkpoint at {ckpt_save_path}')\n",
    "            self.counter = 0        \n",
    "            return (False,self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading..: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 1443/1443 [07:24<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss 254.2183 val_Loss 233.1651 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading..: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 1442/1442 [07:39<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss 200.2042 val_Loss 252.3810 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading..:   4%|âââ                                                                  | 61/1441 [00:21<07:57,  2.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-2f637aa72160>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mfeatures_masked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmiss_vals_masked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmiss_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         loss, pred, attention_weights = train_step(country, year, weekday, hour, features, features_masked,\n\u001b[0m\u001b[0;32m     40\u001b[0m                                                 miss_vals_masked, pos_enc, feature_nr, mask)\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-0d521a9c0ef1>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(country, year, weekday, hour, features, features_masked, miss_vals_masked, pos_enc, feature_nr, mask)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1078\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[0;32m   1079\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1080\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1081\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MeanGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    267\u001b[0m     factor = _safe_shape_div(\n\u001b[0;32m    268\u001b[0m         math_ops.reduce_prod(input_shape), math_ops.reduce_prod(output_shape))\n\u001b[1;32m--> 269\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruediv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mtruediv\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m   \"\"\"\n\u001b[1;32m-> 1336\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_truediv_python3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_truediv_python3\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1273\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1275\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal_div\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mreal_div\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   7322\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7323\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7324\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   7325\u001b[0m         _ctx, \"RealDiv\", name, x, y)\n\u001b[0;32m   7326\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "losses = [[],[]]\n",
    "scheduler = Scheduler(ckpt, ckpt_manager, learning_rate, patience = 5, delta = 5, fine_tuning=False)\n",
    "\n",
    "for epoch in range(EPOCHS):    \n",
    "\n",
    "    #Create a smaller sample of the training data for computational purposes\n",
    "    #Only while testing the model\n",
    "    #index = random.choices(range(training_data.shape[0]),k=1000)\n",
    "    #train_res = training_data[index]\n",
    "    \n",
    "    #Shuffle training data\n",
    "    #np.random.shuffle(training_data)\n",
    "    \n",
    "    # split train data in train and val sets\n",
    "    samples_train, samples_val = val_split(training_data,split=0.2)\n",
    "\n",
    "    # create train dataset\n",
    "    data_train = tf.data.Dataset.from_tensor_slices(samples_train)\n",
    "    data_train = data_train.cache()\n",
    "    data_train = data_train.shuffle(samples_train.shape[0]).padded_batch(BATCH_SIZE) #, drop_remainder=True\n",
    "    data_train = data_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # create val dataset \n",
    "    data_val = tf.data.Dataset.from_tensor_slices(samples_val)\n",
    "    data_val = data_val.cache()\n",
    "    data_val = data_val.shuffle(samples_val.shape[0]).padded_batch(BATCH_SIZE) #, drop_remainder=True\n",
    "    data_val = data_val.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    \n",
    "    \n",
    "    # train the model\n",
    "    for (batch, data_inp) in tqdm(enumerate(data_train), desc = \"Loading..\", total = int(np.ceil(samples_train.shape[0]/BATCH_SIZE))):\n",
    "        features, miss_vals, pos_enc, country, year, weekday, hour, feature_nr = change_format(data_inp)\n",
    "        features_masked, miss_vals_masked, mask = mask_features(features, miss_vals)\n",
    "\n",
    "        loss, pred, attention_weights = train_step(country, year, weekday, hour, features, features_masked,\n",
    "                                                miss_vals_masked, pos_enc, feature_nr, mask)\n",
    "\n",
    "        train_loss(loss)\n",
    "    losses[0].append(train_loss.result())\n",
    "\n",
    "\n",
    "    # validation data calculation\n",
    "    for (batch, data_inp) in enumerate(data_val):\n",
    "        features, miss_vals, pos_enc, country, year, weekday, hour, feature_nr= change_format(data_inp)\n",
    "        features_masked, miss_vals_masked, mask = mask_features(features, miss_vals)\n",
    "\n",
    "        pred,_ = transformer(country, year, weekday, hour, features_masked, miss_vals_masked,\n",
    "                            pos_enc, feature_nr, training=False)\n",
    "        loss = loss_function(features, pred, mask)\n",
    "        val_loss(loss)\n",
    "    losses[1].append(val_loss.result())\n",
    "\n",
    "    print ('Epoch {} - Loss {:.4f} val_Loss {:.4f} '.format(\n",
    "        epoch + 1, train_loss.result(), val_loss.result()))\n",
    "\n",
    "    #Check early stopping and checkpointing\n",
    "    stopping, learning_rate = scheduler(float(val_loss.result()), epoch)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    if stopping == True:\n",
    "        if ckpt_manager.latest_checkpoint:\n",
    "            ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "            print(\"Training finished\")      \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b0463e4f27a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[0;32m   2374\u001b[0m     \"\"\"\n\u001b[0;32m   2375\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2376\u001b[1;33m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[0;32m   2377\u001b[0m                        \u001b[1;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2378\u001b[0m                        \u001b[1;34m'`fit()` with some data, or specify '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[0])\n",
    "plt.plot(losses[1])\n",
    "plt.legend([\"Training loss\",\"Validation loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test data\n",
    "\n",
    "Modell wird noch einmal auf finalen Testdatensatz angewandt. Dann kann der Testdatensatz extrahiert werden um ihn fÃ¼r die anderen Modelle zum Benchmarking zu benutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 32\n",
    "data_test = tf.data.Dataset.from_tensor_slices(testing_data)\n",
    "data_test = data_test.cache()\n",
    "data_test = data_test.padded_batch(BATCH_SIZE)\n",
    "data_test = data_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing process: Testing mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for testing mask\n",
    "def get_testing_mask(features, miss_vals, mask):\n",
    "    mask = mask.reshape(features.shape)\n",
    "    features_masked = np.array(features)\n",
    "    features_masked[mask==1] = 0\n",
    "\n",
    "    miss_vals_masked = np.array(miss_vals)\n",
    "    miss_vals_masked[mask==1] = 1\n",
    "\n",
    "    \n",
    "    return features_masked, miss_vals_masked, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading..: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 173/173 [00:18<00:00,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04958238407081693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Load testing mask\n",
    "testing_mask=np.load(\"../evaluation/testing_mask_test.npy\")\n",
    "#Create empty arrays for Results\n",
    "prediction = np.array([])\n",
    "values_act=np.array([])\n",
    "\n",
    "\n",
    "for (batch, data_inp) in tqdm(enumerate(data_test), desc = \"Loading..\", total = int(np.ceil(testing_data.shape[0]/BATCH_SIZE))):\n",
    "    features, miss_vals, pos_enc, country, year, weekday, hour, feature_nr = change_format(data_inp)\n",
    "    features_masked, miss_vals_masked, mask = get_testing_mask(features, miss_vals, testing_mask[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE])\n",
    "\n",
    "    pred,attention_weights = transformer(country, year, weekday, hour, features_masked, miss_vals_masked, \n",
    "                        pos_enc, feature_nr, training=False)\n",
    "\n",
    "    #Add real values and prediction to dataframe\n",
    "    values_act = np.append(values_act,features[mask==1].flatten())\n",
    "    prediction = np.append(prediction,pred[mask==1].numpy())\n",
    "\n",
    "#Print mse\n",
    "mse = mean_squared_error(values_act,prediction)\n",
    "print(mse)\n",
    "\n",
    "np.save(\"../predictions/transformer_pred_test\",prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and predict testing masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading..: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 173/173 [00:16<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0059845763500354894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading..: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 173/173 [00:16<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00802078605074106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading..: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 173/173 [00:16<00:00, 10.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010592143370899365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading..: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 173/173 [00:16<00:00, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019093774936573348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for p in [0.2,0.4,0.6,0.8]:\n",
    "    testing_mask=np.load(\"../evaluation/testing_mask_{}.npy\".format(p))\n",
    "    \n",
    "    #Create empty arrays for Results\n",
    "    values_act=np.array([])\n",
    "    prediction = np.array([])\n",
    "\n",
    "    for (batch, data_inp) in tqdm(enumerate(data_test), desc = \"Loading..\", total = int(np.ceil(testing_data.shape[0]/BATCH_SIZE))):\n",
    "        features, miss_vals, pos_enc, country, year, weekday, hour, feature_nr= change_format(data_inp)\n",
    "        features_masked, miss_vals_masked, mask = get_testing_mask(features, miss_vals, testing_mask[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE])\n",
    "\n",
    "        pred,attention_weights = transformer(country, year, weekday, hour, features_masked, miss_vals_masked, \n",
    "                            pos_enc, feature_nr, training=False)\n",
    "\n",
    "        #Add real values and prediction to dataframe\n",
    "        values_act = np.append(values_act,features[mask==1].flatten())\n",
    "        prediction = np.append(prediction,pred[mask==1].numpy())\n",
    "        \n",
    "    #Calculate mse\n",
    "    mse = mean_squared_error(values_act,prediction)\n",
    "    print(mse)\n",
    "    \n",
    "    #Save prediction\n",
    "    np.save(\"../predictions/transformer_pred_{}\".format(p),prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
